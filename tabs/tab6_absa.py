import streamlit as st
import pandas as pd
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import TfidfVectorizer
from konlpy.tag import Okt
import openai
import os

# âœ… OpenAI í´ë¼ì´ì–¸íŠ¸
try:
    client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    gpt_enabled = True
except Exception:
    gpt_enabled = False

# í˜•íƒœì†Œ ë¶„ì„ í•¨ìˆ˜
def tokenize(text: str) -> str:
    okt = Okt()
    return ' '.join(okt.nouns(text)) if isinstance(text, str) else ''

# í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨í•œ ë¶„ë¥˜ (Fallback)
def summarize_topic_keywords(keywords: list[str]) -> str:
    joined = ' '.join(keywords)
    if any(word in joined for word in ['ë°°ì†¡', 'ë¹ ë¦„', 'ì •í™•', 'í•˜ë£¨']):
        return 'ğŸ“¦ ë°°ì†¡ ê´€ë ¨'
    elif any(word in joined for word in ['ë§›', 'ê³ ì†Œ', 'ë‹¬ì½¤', 'ì§„í•˜ë‹¤', 'ì´ˆì½”']):
        return 'ğŸ« ë§›ê³¼ í–¥ë¯¸'
    elif any(word in joined for word in ['ê°€ê²©', 'ì €ë ´', 'ë¹„ì‹¸ë‹¤', 'í• ì¸']):
        return 'ğŸ’° ê°€ê²© ì´ìŠˆ'
    elif any(word in joined for word in ['í¬ì¥', 'ê¹”ë”', 'ê¹¨ë—', 'ìƒì']):
        return 'ğŸ í¬ì¥ ìƒíƒœ'
    else:
        return 'ğŸŒ€ ê¸°íƒ€'

# GPT ì£¼ì œ ë¼ë²¨ë§ í•¨ìˆ˜ (ì˜¤ë¥˜ ì‹œ fallback ì‚¬ìš©)
def get_topic_label(keywords: list[str]) -> str:
    if not gpt_enabled:
        return summarize_topic_keywords(keywords)

    prompt = f"""
ë‹¤ìŒ í‚¤ì›Œë“œë“¤ì„ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” ì£¼ì œë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.
í‚¤ì›Œë“œ: {', '.join(keywords)}
í˜•ì‹: í•˜ë‚˜ì˜ ì£¼ì œ ì´ë¦„ë§Œ ì¶œë ¥ (ì˜ˆ: 'ê°€ê²© í‰ê°€', 'ë§›', 'ë°°ì†¡ ê²½í—˜')
"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5
        )
        return response.choices[0].message.content.strip()
    except Exception:
        return summarize_topic_keywords(keywords)

# ì „ì²´ ë¶„ì„ í•¨ìˆ˜
def render(df: pd.DataFrame, n_topics: int = 4, n_keywords: int = 10):
    st.title("ABSA ì£¼ì œ ë¶„ì„")
    st.subheader('**ì£¼ì œ ì´ë¦„ì€ ì°¸ê³ ë§Œ ë¶€íƒ ë“œë¦½ë‹ˆë‹¤**')

    selected_brand = st.selectbox("ë¸Œëœë“œë¥¼ ì„ íƒí•˜ì„¸ìš”", df.keys())
    df = df[selected_brand]

    if 'ë¦¬ë·° ë‚´ìš©' not in df.columns:
        st.error("â— 'ë¦¬ë·° ë‚´ìš©' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.write("ì»¬ëŸ¼ ëª©ë¡:", df.columns.tolist())
        return

    df = df.copy()
    df['í˜•íƒœì†Œ'] = df['ë¦¬ë·° ë‚´ìš©'].astype(str).apply(tokenize)

    vectorizer = TfidfVectorizer(max_df=0.9, min_df=2)
    dtm = vectorizer.fit_transform(df['í˜•íƒœì†Œ'])

    if dtm.shape[0] == 0 or dtm.shape[1] == 0:
        st.warning("â— ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
    lda.fit(dtm)

    vocab = vectorizer.get_feature_names_out()
    topic_data = []

    st.info("ğŸ” ì£¼ì œ ì¶”ì¶œ ì¤‘...")

    for idx, topic in enumerate(lda.components_):
        keywords = [vocab[i] for i in topic.argsort()[:-n_keywords - 1:-1]]
        label = get_topic_label(keywords)

        topic_data.append({
            "ì£¼ì œ ë²ˆí˜¸": f"ì£¼ì œ {idx+1}",
            "ì£¼ì œ ì´ë¦„": label,
            "í‚¤ì›Œë“œ ëª©ë¡": ', '.join(keywords)
        })

    st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
    st.table(pd.DataFrame(topic_data))

# # ì‹¤í–‰ ë¶€ë¶„
# if __name__ == "__main__":
#     st.set_page_config(layout="wide")

#     st.sidebar.header("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")
#     uploaded_file = st.sidebar.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ (ë¦¬ë·° ë‚´ìš© í¬í•¨)", type="csv")

#     if uploaded_file:
#         df = pd.read_csv(uploaded_file)
#         run_absa(df)
#     else:
#         st.warning("ì™¼ìª½ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
